---
title: "Spew Development Strategy and Documentation"
author: "Lee Richardson, Sam Ventura, Shannon Gallagher"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction 

This document is meant to serve as both the documentation for specific aspects of spew as well as notes on our general guidelines for development. The hope is that is can help explain what we're doing at a more high level than having to dig through the source code to get to the bottom of what's happening. In this, a set of guidelines, specific examples, and high level explanations about the program will be documented. 

Most of what I've learned about specifically developing [R Packages](http://r-pkgs.had.co.nz/) comes from the Hadley Wickham book, so for more details on the components described below, a good reference is either this or the official [R extensions](https://cran.r-project.org/doc/manuals/R-exts.html#Creating-R-packages) manual. Another good refererence is John Chambers: Programming with Data: Chapter 4. 

Developing R packages is a lot smoother when using the **devtools** package, along with a few others. To install the key packages we will be using, use:

```{r, eval=FALSE}
install.packages(c("devtools", "roxygen2", "testthat", "knitr"), 
                 repos = "http://cran.rstudio.com/")
```

# High Level Principles 

This section of the document contains the high level principles we plan on sticking to while developing spew. Of course, some of these are more idealizations than what will happen in practice, as deadlines, requests, etc... can cause us to abandon best practices. However, we want to work hard to follow these high level principles, as it will make spew a more reliable program, document everything we are doing, and overall just make our lives easier and help us learn more about programming.  

## Style 

It's important to agree on a coding style guide up front when creating a project with multiple contributors. This will make spew easier read, understand, verify, build on. Since we are writing spew in R, we will use the [Advanced R](http://adv-r.had.co.nz/Style.html) style guide, which integrates well with the books we are basing our package structure around. 

## Testing 

In the past version's of our synthetic population generator, we've had issues when we tweaked certain component to generate a specific population, only to learn that this would break something down the road. This lead to a general anxiety when adding new features to the code, and less confidence in our ability to quickly generate new populations with various subtleties compared with the one's which already worked.  

In spew, we are hoping to alleviate these concerns by making unit testing a routine part of the development process. Ideally, every function which is used will have a corresponding test associated with it. That way, we can be confident that making a change won't mess things up down the road. Another benefit of this is that since we are testing individual level functions, this will force us to break spew up into smaller pieces, making it easier to both combine them together and reducing linkages between components. 

Ideally, we can tests whenever we find ourselves debugging spew and checking out the performance of new functions. This way, all of the ad-hoc tests will be automated. In a later section, we will have an example of writing tests for the `read.R` series of functions, using the `testthat` package.  

## Adding documentation to functions 
Built into R is a standard way of documenting objects in a package. And since everything in R is an object (including functions!), we will utilize this to document the functions in our package. The way it works is that you create `.Rd` files in the `man/` directory in the package, which is required to be a certain format. If the `.Rd` file is in the correct format, then R can produce  html, plaintext, or PDF documentation that we are used to using and is so helpful. Building this documentation system into the language itself is a tremendously helpful part of R.  

Since it's tedious to create a `.Rd` file for each individual function, we will use the `roxygen2` package for documentation to speed this up. This allows us to put the documentation right on top of our functions. Note that we were intuitively doing this in the last versions of our program, but utilizing roxygen will give us so all we need to to is type:

```{r, eval=FALSE}
  devtools::document()
```

and the latest documentation (`.Rd` files) are created for us, using the required syntax. Note that this allows us to get aorund things like imposing our own commenting structure, as we can just use the conventions of R. This not only removes the burden of trying to create our own system, but if all of our functions have appropriate documentation, all we will need to do is type `?function_name` and we can see all of the parameters used in various functions. 

For example, right now I'm writing the `make_data` function. This function is the final workhorse for our program, as it takes in all of the formattted inputs and writes a csv of microdata for each one of our countries. Let's say we want to specify the inputs and outputs of our mkae function, and give a few examples of how to call this using our example data-sets. 

```{r}
#' Create microdata using formatted data 
#' 
#' @param pop_table dataframe with columns corresponding to 
#' which places need populations, and how many samples to take 
#' @param sp class object shapefile used for assigning households to 
#' particular locations  
#' @param dataframe with microdata corresponding to housegolds 
#' @param dataframe with microdata corresponding to people 
#' @param logical indicating whether or not we will generate our 
#' synthetic populations in parallel
#' @param character vector indicating the type of sample to use for 
#' generating microdata 
#' @return logical specifying whether the microdata was generated 
#' successfully 
#' @examples
#'  make_data(sd_data$pop_table, sd_data$shapefiles, sd_data$pums$pums_h, sd_data$pums$pums_p)
make_data <- function(pop_table, shapefile, pums_h, pums_p, 
                      parallel = FALSE, sampling_type = "uniform") {
  
  num_places <- nrow(pop_table) 

  for (place in 1:num_places) {
    households <- sample_households(pop_table[place, "n_house"], 
                                    pop_table[place, "puma_id"], 
                                    pums_h)
  }

}
```

Then, all we need to do is run roxygen, and the documentation will be made available to us in R's familiar documentation format. 

```{r}
  devtools::document()
  ?make_data
```

## Version Control  


## Code Reviews 
  

# Common Tasks 


## Writing tests for functions 

In developing spew, we will make use of the `testhat` package to write unit tests for our  functions. This gives us a method for automating tests with a pre defined structure, where all of the tests are located in the *tests/testthat* directory. To run all of our tests, we can use the command `devtools::test()`. This will run all of the tests in this folder, display which one's passed, and provide error messages if they fail. Ideally, we should run these tests before making changes to spew. 

One of the main parts for spew is reading in all of the data sources, and making sure that they are in a list for use in later aspects of the program. We want to write a set of  unit tests for our read functions, to make sure that that they are functioning as expected. That way, if we edit our read functions and introduce an error, we will not only know that our functions failed the tests, but also which specific files/functions causes the failure. Note that the beauty of this is that by writing tests for each individual read function, this forces us to break our code into smaller, more modular pieces, making it easier to understand. 

Let's look at how to write a unit test for our `read_pop_table` function. In testthat, we organize our tests hierarchically, where each file contains a set of tests, and each test contains a set of expectations, each of which is ran everytime we test our package. In this case, we have:

1. File - `tests/testthat/test-read.R`. This file will contain all the tests for the read functions. Note that every testing file must start with 'test'
2. Test - Within `test_that("Individual United States functions"` contains all of our tests to ensure our individual read functions work
3. Expectation - For example `expect_equal(nrow(sd_poptable), 222)` contains an expectation that our poptable will have 222 rows. 


Note that by default, unit tests are self contained in the tests/testthat directory, meaning that we need to add two lines to ensure that we are reading in from that correct `data-raw/` directory, where we put the example South Dakota data.

```{r, eval=FALSE}
context("Read Functions")

test_that("Individual United States functions", {
  
  # Make sure we are using the correct data-raw directory 
  # as opposed to the tests/testthat one within the package 
  spew_dir <- system.file("", package = "spew")
  data_path <- paste0(spew_dir, "/", "data-raw/46")
  
  # Pop Table -------------------------------- 
  sd_poptable <- read_pop_table(data_path, 
                                data_group = "US", 
                                folders = list(pop_table = "popTables", 
                                               pums = "pums", 
                                               schools = "schools", 
                                               lookup = "tables", 
                                               shapefiles = "tiger", 
                                               workplaces = "workplaces")) 
  
  # Make sure the reading in of the pop_table works 
  # as we expected 
  expect_equal(nrow(sd_poptable), 222)
  expect_equal(ncol(sd_poptable), 4)
  expect_equal(class(sd_poptable), "data.frame")
  
  # Test that we are reading in characters instead of factors
  expect_equal(any(lapply(sd_poptable, class) == "factor"), FALSE)
}) 
```

This is an example of how we can test one of our individual read functions. If we now run `devtools::test()`, each one of these expectations will be ran, along with a print-out on if they passed, failed, and why. 

It's often hard to come up with what to test. One heurustic I've been using is that whenever I am ad-hoc testing/debugging a function at the command line, I should instead formally do this in the test that package instead. That way, the bug will in some senses be fixed forever, and we won't have to remember this specific condition every time we repeat the same code. 

## Using Example data 


## Git for merging data 


# Detailed Situations 

This section will document some of the more detailed situations we find ourselves in, along with strategies for dealing with them. This is hopefully a section which will be updated often, as it will allow all of us to deal with situations in a unified manner, which will reduce confusion down the road. 

## Adding non package-related files/directories (.Rbuildignore)

There are times when we need specific files for spew, however there is no good place to put them that fit's into the R package structure. Thankfully, we can get around these issues using the `.Rbuildignore` file, which is included in the package. 

The way this works is that the code we have hosted on Github is known as the *source* version of our package. In order for users to ultimately use our package, and not just development version, the source packages needs to be built, converted to binary, and then installed. The `.Rbuildignore` file allows you to specify both files and directories which won't be included when the package is installed, hence they are safe to keep in our source (ie: Github) version of the package, and won't be included when users install the package. 

For example, for testing/verification purposes, we want to develop spew using example data from both our United States and International formats of data. We want this to replicate the data format we see on Olympus, that way we can be sure if the functions and synthetic populations are working here, they are sure to work on Olympus. Because the Olympus format of the data isn't formatted the same way we would format a typical R data-set, we can include the data in our source (Github) package, and use `.Rbuildignore` to make sure this data isn't carried through to the insallation process. 

To add the example data, simply use:

```{r}
devtools::use_build_ignore("data-raw")
```

Now, in this folder we can also include scripts which convert this raw data into more efficient `.rdata` files, which can be used for testing/verification of other aspects of spew down the road.  

## Including dependencies in our package 

One critical aspect of developing and R package is learning how to incorporate dependencies from other packages, while not adversely effecting other users' R landscape. For example, in spew, we rely on certain packages for dealing with shapefiles, linking records, etc... Instead of putting `library(sp)` in the first line of the function, the preferred method is to add the `sp` package to our list of dependencies, and this way to will avoid errors down the road. Fortunately, devtools gives an easy way to add dependencies to our `DESCRIPTION` file, which contains all of the metadata needed for using our package. 

To add a necessary package dependency to spew, simply use:

```{r, eval=FALSE}
devtools::use_package("sp")
```

This means that anyone using the spew package MUST have the sp package installed as well. For a less stringest requirement, (ie: only some use cases need the record linkage package), we can simply suggest a dependency:

```{r, eval=FALSE}
devtools::use_package("stringdist", "Suggests")
```

Note for dependendies with other packages, you can use the `::` command, which specifies which package a particular function is coming from. For example, when we use the slot function we can write `methods::slot(shapefile, "polygons")`


# About the Program 

Right now, we have divided spew into three main sections, each of which will be described in this section. The three sections are:

1. Read 
2. Format
3. Make 

## Read
The read.R functions have a clear purpose: read in all the necessary data for a a particular run of spew. Because the data comes in many different formats, places, etc, this is the function which will ideally handle al of these for us. For our purposes, most of the data is in a particular structure on Olympus, so we specify particular data groups as inputs to make everything work easier. But in principal, once should be able to specify filepaths to each data source, and read_data should be able to access them. 

For our program to become a general engine for synthetic population generation, we need it to be able to work for any arbitrary data source people have. To address this, we also have functions in read_data which standardize the data. At a high level, this means that any data source we use needs to have some columns, ID, etc... which allows it to be linked with the other data sources. For spew, the most important thing is that the locations of each data source can be combined, and for this we standardized each data source to have the colums/elements:  

1. place_id 
2. puma_id 

The idea here is that we are generating a population for n individual places, which we specify with the place ID. In some situations, such as the US, the pums data we recieve is more granular, and we can subset it to obtain more accurate populations. We designate this subset with the `puma_id`, which is used later down the road in the `make_data` functions. Thus, every data source coming through `read_data` should not only be loaded into R's memory, but it should also have an identifier indicating which part of it corresponds to the place id, and `puma_id` (if necessary). 


## Format 

The format functions are meant to link the location identifier in each data-source with eachother, and also serve as the last series of checks before the microdata is generated. 

## Make 


# Running SPEW on Olympus 

Since our project is all hosted on github, we should in theory be able to simply install the package on Olympus, and run the `spew` function with the appropriate inputs just as we would with any other package. To do this, we need to have devtools installed (which Jay has nicely done for us) on the headnode. Furthermore, we must make sure that we have the most recent version of R loaded, using the command:

```{r, eval=FALSE}
# module load r/3.2.1
```

This will replace the default R 3.1 with the latest versions. From here, to download spew onto Olympus we type in R to enter an R session, then the following:

Next, we want to update the version of SPEW on Olympus in our personal R libraries with the version of Github which presumably we have changed. To be safe, I remove the current version of SPEW downloaded and then re-install SPEW from Github with the following commands:

```{r, eval=FALSE}
library(devtools)
library(httr)
personal_lib <- .libPaths()[2]
remove.packages(pkgs = "spew", lib = personal_lib)
with_libpaths(new = personal_lib, install_github("leerichardson/spew"))
library(spew)
?generate_spew
```

Once you re-load the package, it's useful to debug it in Interactive mode on Olympus, o make sure things are working as expected. you can submit an interactive job (ie: Log onto one of the compute nodes) with the command `qsub -I -l walltime=3:00:00`. Then we can use the following commands, which are similar to the commands we run when calling SPEW from the command line, to run SPEW for any of our three data groups (just comment out the data-group you are looking to run with here:)

```{r, eval=FALSE}
rm(list = ls())
options(error = recover)

library(spew)
library(methods) 
library(sp)
library(maptools)
library(foreach)
library(doSNOW)
library(stringdist)
library(plyr)
library(rgeos)
library(data.table)
library(bit64)

data_group <- "US"
convert_count = FALSE
folders <- list(pop_table = "popTables", 
                      pums = "pums/2013", 
                      schools = "schools/2013", 
                      lookup = "tables", 
                      shapefiles = "tiger", 
                      workplaces = "workplaces")

	vars = list(household = c("RT", "SERIALNO", "DIVISION", "PUMA", "REGION", "ST", "ADJHSG", "ADJINC", "WGTP", "NP", "TYPE", "ACCESS", "ACR", "AGS", "BATH", "BDSP", "BLD", "BROADBND", "BUS", 
							"COMPOTHX", "CONP", "DIALUP", "DSL", "ELEP", "FIBEROP", "FS", "FULP", "GASP", "HANDHELD", "HFL", "INSP", "LAPTOP", "MHP", "MODEM", "MRGI", 
							"MRGP", "MRGT", "MRGX", "OTHSVCEX", "REFR", "RMSP", "RNTM", "RNTP", "RWAT", "RWATPR", "SATELLITE", "SINK", "SMP", "STOV", 
							"TEL", "TEN", "TOIL", "VACS", "VALP", "VEH", "WATP", "YBL", "FES", "FFINCP", "FGRNTP", "FHINCP", "FINCP", "FPARC", "FSMOCP",
							"GRNTP", "GRPIP", "HHL", "HHT", "HINCP", "HUGCL", "HUPAC", "HUPAOC", "HUPARC", "KIT", "LNGI", "MULTG", "MV", "NOC", "NPF", 
							"NPP", "NR", "NRC", "OCPIP", "PARTNER", "PLM", "PSF", "R18", "R60", "R65", "RESMODE", "SMOCP", "SMX", "SRNT", "SSMC", "SVAL", 
							"TAXP", "WIF", "WKEXREL", "WORKSTAT", "FACCESSP", "FACRP", "FAGSP", "FBATHP", "FBDSP", "FBLDP", "FBROADBNDP", "FBUSP", "FCOMPOTHXP", "FCONP", 
							"FDIALUPP", "FDSLP", "FELEP", "FFIBEROPP", "FFSP", "FFULP", "FGASP", "FHANDHELDP", "FHFLP", "FINSP", "FKITP", "FLAPTOPP", "FMHP", "FMODEMP", 
							"FMRGIP", "FMRGP", "FMRGTP", "FMRGXP", "FMVP", "FOTHSVCEXP", "FPLMP", "FREFRP", "FRMSP", "FRNTMP", "FRNTP", "FRWATP", "FRWATPRP", 
							"FSATELLITEP", "FSINKP", "FSMP", "FSMXHP", "FSMXSP", "FSTOVP", "FTAXP", "FTELP", "FTENP", "FTOILP",
							"FVACSP", "FVALP", "FVEHP", "FWATP", "FYBLP"),
				person = c("RT", "SERIALNO", "SPORDER", "PUMA", "ST", "ADJINC", "PWGTP", "AGEP", "CIT", "CITWP", "COW", "DDRS", "DEAR", "DEYE", "DOUT", "DPHY", "DRAT", "DRATX", "DREM", "ENG", 
							"FER", "GCL", "GCM", "GCR", "HINS1", "HINS2", "HINS3", "HINS4", "HINS5", "HINS6", "HINS7", "INTP", "JWMNP", "JWRIP", "JWTR", "LANX", "MAR", "MARHD", "MARHM", 
							"MARHT", "MARHW", "MARHYP", "MIG", "MIL", "MLPA", "MLPB", "MLPCD", "MLPE", "MLPFG", "MLPH", "MLPI", "MLPJ", "MLPK", "NWAB", "NWAV", "NWLA", "NWLK", "NWRE", "OIP", 
							"PAP", "RELP", "RETP", "SCH", "SCHG", "SCHL", "SEMP", "SEX", "SSIP", "SSP", "WAGP", "WKHP", "WKL", "WKW", "WRK", "YOEP", "ANC", "ANC1P", "ANC2P", "DECADE", "DIS", 
							"DRIVESP", "ESP", "ESR", "FOD1P", "FOD2P", "HICOV", "HISP", "INDP", "JWAP", "JWDP", "LANP", "MIGPUMA", "MIGSP", "MSP", "NAICSP", "NATIVITY", "NOP", "OC", "OCCP", 
							"PAOC", "PERNP", "PINCP", "POBP", "POVPIP", "POWPUMA", "POWSP", "PRIVCOV", "PUBCOV", "QTRBIR", "RAC1P", "RAC2P", "RAC3P", "RACAIAN", "RACASN", "RACBLK", 
							"RACNH", "RACNUM", "RACPI", "RACSOR", "RACWHT", "RC", "SCIENGP", "SCIENGRLP", "SFN", "SFR", "SOCP", "VPS", "WAOB", "FAGEP", "FANCP", "FCITP", "FCITWP", 
							"FCOWP", "FDDRSP", "FDEARP", "FDEYEP", "FDISP", "FDOUTP", "FDPHYP", "FDRATP", "FDRATXP", "FDREMP", "FENGP", "FESRP", "FFERP", "FFODP", "FGCLP", "FGCMP", 
							"FGCRP", "FHINS1P", "FHINS2P", "FHINS3C", "FHINS3P", "FHINS4C", "FHINS4P", "FHINS5C", "FHINS5P", "FHINS6P", "FHINS7P", "FHISP", "FINDP", "FINTP", "FJWDP", 
							"FJWMNP", "FJWRIP", "FJWTRP", "FLANP", "FLANXP", "FMARHDP", "FMARHMP", "FMARHTP", "FMARHWP", "FMARHYP", "FMARP", "FMIGP", "FMIGSP", "FMILPP", "FMILSP", "FOCCP", 
							"FOIP", "FPAP", "FPERNP", "FPINCP", "FPOBP", "FPOWSP", "FPRIVCOVP", "FPUBCOVP", "FRACP", "FRELP", "FRETP", "FSCHGP", "FSCHLP", "FSCHP", "FSEMP", "FSEXP", "FSSIP", 
							"FSSP", "FWAGP", "FWKHP", "FWKLP", "FWKWP", "FWRKP"))
# vars = list(household = NA, person = NA)
sampling_method <- "uniform"
locations_method <- "roads"
in_dir <- "/mnt/beegfs1/data/shared_group_data/syneco/input/west/north_america/united_states/06/input"
out_dir <- "/mnt/beegfs1/data/shared_group_data/syneco/test/united_states/06/"

# data_group = "ipums"
# convert_count = TRUE
# folders <- list(pop_table = "counts", 
#                     pums = "PUMS", 
#                     shapefiles = "shapefile_ipums")
# vars = list(household = c("COUNTRY","YEAR","SAMPLE","SERIAL","PERSONS","HHWT",
#                             "FORMTYPE","REGIONW","GEOLEV1","GEOLEV2","HHTYPE",
#                             "PERNUM","PERWT","RELATE","RELATED"), 
#             person = c("SERIAL","AGE","SEX","RACE","SCHOOL","INCTOT"))
# sampling_method <- "uniform"
# locations_method <- "uniform"
# in_dir <- "/mnt/beegfs1/data/shared_group_data/syneco/input/east/asia/bangladesh/input"
# out_dir <- "/mnt/beegfs1/data/shared_group_data/syneco/test/bangladesh/"

# data_group <- "none"
# convert_count = FALSE
# folders = list(pop_table = "/mnt/beegfs1/data/shared_group_data/syneco/input/west/north_america/canada/input/counts/canada_pop_table.csv", 
#                      pums = list(pums_h = "/mnt/beegfs1/data/shared_group_data/syneco/input/west/north_america/canada/input/pums/pums_h.csv", 
#                                  pums_p = "/mnt/beegfs1/data/shared_group_data/syneco/input/west/north_america/canada/input/pums/pums_p.csv"),  
#                      shapefiles = "/mnt/beegfs1/data/shared_group_data/syneco/input/west/north_america/canada/input/shapefiles/canada_shapefiles.shp")
# 
# vars = list(household = c("SERIALNO", "puma_id"), 
#   			    person = c("SERIALNO", "EF_ID","CF_ID","PP_ID","ABOID","AGEGRP","AGEIMM","ATTSCH","BFNMEMB","BEDRM","BUILT","CF_RP","CFSTAT","CFSTRUCT","CIP2011","CITIZEN", "CONDO","COW","DIST","DTYPE","DUR","EF_RP","EFDECILE","EFDIMBM","EMPIN","ETHDER","FCOND","FOL","FPTWK","GENSTAT","GROSRT","GTRFS","HDGREE","HHMAINP","HLAEN","HLAFR","HLANO","HLBEN","HLBFR","HLBNO", "HRSWRK","IMMSTAT","INCTAX","KOL","LEAVE","LFTAG","LOC_ST_RES","LOCSTUD","LOLICOA","LOLICOB","LOLIMA","LOLIMB", "LOLIMMI","LOMBM","LSTWRK","LWAEN","LWAFR","LWANO","LWBEN","LWBFR","LWBNO","MARSTH","MOB1","MOB5","MODE", "MRKINC","MTNEN","MTNFR","MTNNO","NAICS","NOCEE","NOCS","NOL","NOS","OCC","OMP","POB","POBF","POBM","POWST", "PR","PR1","PR5","PRESMORTG","PRIHM","PWPR","REGIND","RELIGION","REPAIR","ROOM","SEX","SSGRAD","SUBSIDY","TENUR", "TOTINC","TOTINC_AT","VALUE","VISMIN","WEIGHT","WKSWRK","WRKACT","WT1","WT2","WT3","WT4","WT5","WT6","WT7","WT8","YRIMM"))
# sampling_method <- "uniform"
# locations_method <- "uniform"
# in_dir <- "/mnt/beegfs1/data/shared_group_data/syneco/input/west/north_america/canada/input"
# out_dir <- "/mnt/beegfs1/data/shared_group_data/syneco/test/canada/"

generate_spew(input_dir = in_dir, 
        			folders = folders, 
      				data_group = data_group, 
      				output_dir = out_dir, 
      				convert_count = convert_count, 
      				parallel = TRUE, 
      				sampling_method = sampling_method,  
      				locations_method = locations_method,
      				vars = vars)
```

## Shell Scripting 
In our use of spew, we will make a lot of use of the Olympus infrastructure to speed up our calculations. In order to use these capabilities, we will need to submit jobs to olympus which request the requisite amount of processors and time. To submit jobs and interact with Olympus, we make use of shell scripts. 

Right now, all of the scripts are located in the `/data/shared_group_data/syneco/spew` directory. All of the files in here have a specific purpose, but the main idea is this. There's a lookup table in the `/input/spew_lookup.csv` directory, which contains all of the inputs necessary to run spew. What we want to do, is to subset this lookup table corresponding to whichever synthetic population we are interested in generated, grab the inputs, and then use these inputs to call spew. This is what the `/spew/run_spew.sh` does, we pass it a query, and this grabs the appropriate inputs from the lookup table. Then, it submits a job for each row of the lookup table that remains after the subset.  

For example, to run a particular state, we can call:
`/data/shared_group_data/syneco/spew/run_spew.sh "which(lookup[, 'state_dirs'] == '56')"`

And to run the entire United States, we can run:
`/data/shared_group_data/syneco/spew/run_spew.sh "which(lookup[, 'country'] == 'united_states')"`

To run South America, use:
`/data/shared_group_data/syneco/spew/run_spew.sh "which(lookup[, 'continent'] == 'united_states')"`

In the case of the United States, what happens is that we subset the lookup table for all of the states, and then submit a job which calls spew for each individual state. The result of these calls to spew are saved in the `/outputs/spew/` directory. 

This system can definitely be improved, and will for sure undergo improvements as we move forward. Ideally, we can keep adding to the spew package ways of handling new types of data, update the parameters necesary for these calls in the lookup table, and systematically generate synthetic populations in the same way for every location upon which we have data. 

# Data 

In principle, spew needs three things in order to generate synthetic ecosystems:

1. Microdata-  This is what we are sampling from, and what the final form of our synthetic ecosystems will look like
2. Counts- Tells us how many people and households to generate for each place 
3. Shapefiles- The shape of the region we are generating a synthetic ecosystem for, allowing us to put agents in specific locations. 

Everything else: Workplaces, schools, vectors, etc... can be added in, but they are not crititcal to getting the synthetic population forumlated. We don't actually *need* shapefiles to generate synthetic populations, but for our purposes we are making sure we have locations attached to individual people and households. Ideally, we will have as much information about the agents as desired for modeling, but are limited by the data we have to generate them. 

So far, our data comes from a few particular sources, which we have classified into `groups`. The idea behind these groups is that the data sources have the same structure, which allows us to standardize the code we use to read and format them. The details of the groups are described next


## United States 


## IPUMS 
While searching for sources with the data of our three necessary ingredients to spew, we came across three which had global coverage we needed. The three sources are:

1. IPUMS- Microdata for international (non-US) countries 
2. Geohive- Population counts for countries. 
3. GADM- Shapefile's for countries

Of course, the main difference between these three sources and the United States data is that these three organizations were not collaborating together, hence the sources are not `harmonized`. Although the data is not perfectly syncronized, the key feature is that each source has a file (or files) corresponding to many individual countries. This means we can use the same code to generate synthetic populations for any country which data from these three sources, hence the reason why we made IPUMS a datagroup of its own. 

The data, code and documenation for these three source is located in the `/data/shared_group_data/syneco/getting_data` directory on Olympus. Note that I made some approximations in moving the data to specific places, and it would be smart to go back over this with a "fine toothed comb" and make sure I'm not missing any obvious countries. 

## Canada 
Upon the request of our MIDAS collaborators, we have obtained the Canadian PUMS data file in order to generate microdata. In addition, we have obtained an individual shapefile from Montreal as well as population counts. We are putting these resources together in order to generate an as realistic as possible synthetic population for Montreal. 

In terms of the PUMS for canada, the household level data is contained in a "Hierarchical" data-set, whereas the individual level data is in the individual data-set. The data-set contains 133,192 private households and 333,008 individual records. Note that the NHS (similar to the ACS) in the USA, is what was used to obtain this microdata, and it's a voluntary self administered survey. Also similar to PUMA's in the USA, the Canadian's were split into "Census metropolitan areas (CMA's)". This should serve as our "PUMA ID" for the Canadian data. Note that the Montreal CMA is "462", and corresponds to around 3.7 million people. 

## Specific country level notes 

In this section, I'm going to jot down country specific notes which we will include in the final documentation. 

### Chile:
  Note that Chile has a duplicate name in its shapefile. For this reason, I ended up just using the first region and the second. 
  
### Colombia:
  Geohive has the "Capital District of Santa Fe de Bogota", but the shapefile does not. For this reason, I took it out and re allocated the remaining people evenly in the other cuntries. Using the 'allocate count' function
