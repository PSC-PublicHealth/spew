---
title: "Spew Development Strategy and Documentation"
author: "Lee Richardson, Sam Ventura, Shannon Gallagher"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction 

This document is meant to provide a set of guidelines and strategies for developing the spew R package. It's meant to document the philosophy behind our approach, along with a set of specific examples and detailed documentation for the most common development tasks that we will encounter. It's meant to be a dynamic set of guidelines, which will adapt as we move forward and learn more about details of spew and what we need in order to implement it successfully. 

Most of what I've learned about specifically developingg [R Packages](http://r-pkgs.had.co.nz/) comes from the Hadley Wickham book, so for more details on the components described below, a good reference is either this or the official [R extensions](https://cran.r-project.org/doc/manuals/R-exts.html#Creating-R-packages) manual. That said, developing R packages is a lot smoother when using the **devtools** package, along with a few others. To install the key packages, use:

```{r, eval=FALSE}
install.packages(c("devtools", "roxygen2", "testthat", "knitr"), 
                 repos = "http://cran.rstudio.com/")
```

# High Level Principles 

This section of the document contains the high level principles we plan on sticking to while developing spew. Of course, some of these are more idealizations than what will happen in practice, as deadlines, requests, etc... can cause us to abandon best practices in order to meet a given task. However, we want to work hard to follow these high level principles, as it will help us build spew more effectively, and we will all learn to become better programmers along the way.  

## Style 

  It's important to agree on a coding style guide up front when creating a project with multiple contributors. This will make spew easier read, understand, verify, and ultimiately contribute to. Since we are writing spew in R, we will start by using the [Advanced R](http://adv-r.had.co.nz/Style.html) style guide, which integrates well with the books we are basing our package structure around. 

## Testing 

In the past version's of our synthetic population generator, we've had issues when we tweaked certain component to generate a specific population, only to learn that this would break something down the road. This lead to a general anxiety when adding new features to the code, and less confidence in our ability to quickly generate new populations with various subtleties compared with the one's which already worked.  

In spew, we are hoping to alleviate some of these concerns by making unit testing a fundamental part of the development process. Unit tests are universally used by professional software engineers, and while they add a little bit of time to the development process, they will save us much time and energy down the road. 

For one, making sure that all the components in spew have a test associared with it will give us a more modular structure. It will help us remove inderdependencies between functions, and alert us to cases when changing one component will adversely effect another. This will also help boost confidence in our code, since if we know that our functions have gone through their associated tests, we can be confident the new features we add aren't introducing bugs down the road. 

Ideally, we can tests whenever we find ourselves checking the results of various functions at the command line. This way, all of the ad-hoc tests we perform can be codified, and we can catch when strange errors persist in our code. In a later section, we will have an example of writing tests for the `read.R` series of functions. 

## Adding documentation to functions 
Built into R is a standard way of documenting objects in a package. And since everything in R is an object, this is what we will use to document all of our functions. The way it works is that you create `.Rd` files in the `man/` directory in the package, which are required to be a certain format. If the `.Rd` format is specified correctly, then R can product, html, plaintext, or PDF documentation that we are so used to viewing. 

Since it's tedious to create a `.Rd` file for each individual function, we will use the `roxygen2` package for documentation. This allows us to put the documentation right above our functions, so all we need to to is type:

```{r, include=FALSE}
  devtools::document()
```

and the latest documentation (`.Rd` files) are created for us, using the required syntax. Note that this allows us to get aorund things like imposing our own commenting structure, as we can just use the conventions of R. This not only removes the burden of trying to create our own system, but if all of our functions have appropriate documentation, all we will need to do is type `?function_name` and we can see all of the parameters used in various functions. 

For example, right now I'm writing the `make_data` function. This function is the final workhorse for our program, as it takes in all of the formattted inputs and writes a csv of microdata for each one of our countries. Let's say we want to specify the inputs and outputs of our mkae function, and give a few examples of how to call this using our example data-sets. 

```{r}
#' Create microdata using formatted data 
#' 
#' @param pop_table dataframe with columns corresponding to 
#' which places need populations, and how many samples to take 
#' @param sp class object shapefile used for assigning households to 
#' particular locations  
#' @param dataframe with microdata corresponding to housegolds 
#' @param dataframe with microdata corresponding to people 
#' @param logical indicating whether or not we will generate our 
#' synthetic populations in parallel
#' @param character vector indicating the type of sample to use for 
#' generating microdata 
#' @return logical specifying whether the microdata was generated 
#' successfully 
#' @examples
#'  make_data(sd_data$pop_table, sd_data$shapefiles, sd_data$pums$pums_h, sd_data$pums$pums_p)
make_data <- function(pop_table, shapefile, pums_h, pums_p, 
                      parallel = FALSE, sampling_type = "uniform") {
  
  num_places <- nrow(pop_table) 

  for (place in 1:num_places) {
    households <- sample_households(pop_table[place, "n_house"], 
                                    pop_table[place, "puma_id"], 
                                    pums_h)
    browser()
  }

}
```

Then, all we need to do is run roxygen, and the documentation will be made available to us in R's familiar documentation format. 

```{r}
  devtools::document()
  ?make_data
```

## Version Control  


## Code Reviews 
  


# Common Tasks 


## Writing tests for functions 

In developing spew, we will make use of the `testhat` package to write unit tests for our corresponding functions. This gives us a nice structure, in which all of the tests are located in the *tests/testthat*. To make sure that our changes pass all of the pre-exising tests, we can use the command `devtools::test()`. This will run all of the tests in this folder, display which one's passed, and provide error messages if they fail. 

For example, one of the main components for spew is reading in all of the data sources, and making sure that they are in a list for use in later components of the program. We want to write a set of complimentary unit tests for our read functions to make sure that we not only have a list with all of the data, but that each file is being read in correctly. That way, if we edit our read functions and introduce an error, we will not only know that our functions failed the tests, but also which specific files/functions causes the failure. 

Let's look at how to write a unit test for our `read_pop_table` function. Note that by default, unit tests are self contained in the tests/testthat directory, meaning that we need to add two lines to ensure that we are reading in from that correct `data-raw/` directory, where we put the example South Dakota data. In testthat, we organize our tests hierarchically, where each file contains a set of tests, and each test contains a set of expectations, each of which is ran everytime we test our package. In this case, we have:

1. File - `tests/testthat/test-read.R`. This file will contain all the tests for the read functions. Note that every testing file must start with 'test'
2. Test - Within `test_that("Individual United States functions"` contains all of our tests to ensure our individual read functions work
3. Expectation - For example `expect_equal(nrow(sd_poptable), 222)` contains an expectation that our poptable will have 222 rows. 


```{r, eval=FALSE}
context("Read Functions")

test_that("Individual United States functions", {
  
  # Make sure we are using the correct data-raw directory 
  # as opposed to the tests/testthat one within the package 
  spew_dir <- system.file("", package = "spew")
  data_path <- paste0(spew_dir, "/", "data-raw/46")
  
  # Pop Table -------------------------------- 
  sd_poptable <- read_pop_table(data_path, data_group = "US", folders = list(pop_table = "popTables", 
                                                                             pums = "pums", 
                                                                             schools = "schools", 
                                                                             lookup = "tables", 
                                                                             shapefiles = "tiger", 
                                                                             workplaces = "workplaces")) 
  # Data frame with the correct dimensions 
  expect_equal(nrow(sd_poptable), 222)
  expect_equal(ncol(sd_poptable), 4)
  expect_equal(class(sd_poptable), "data.frame")
  
  # Making sure stringAsFactors=FALSE 
  expect_equal(any(lapply(sd_poptable, class) == "factor"), FALSE)

}) 
```

This is an example of how we can test one of our individual read functions. If we now run `devtools::test()`, each one of these expectations will be ran, along with a print-out on if they passed, failed, and why. 


## Using Example data 


## Git for merging data 


# Detailed Situations 

This section will document some of the more detailed situations we find ourselves in, along with strategies for dealing with them. This is hopefully a section which will be updated often, as it will allow all of us to deal with situations in a unified manner, which will reduce confusion down the road. 

## Adding non package-related files/directories (.Rbuildignore)

There are times when we need specific files for spew, however there is no good place to put them that fit's into the R package structure. Thankfully, we can get around these issues using the `.Rbuildignore` file, which is included in the package. 

The way this works is that the code we have hosted on Github is known as the *source* version of our package. In order for users to ultimately use our package, and not just development version, the source packages needs to be built, converted to binary, and then installed. The `.Rbuildignore` file allows you to specify both files and directories which won't be included when the package is installed, hence they are safe to keep in our source (ie: Github) version of the package, and won't be included when users install the package. 

For example, for testing/verification purposes, we want to develop spew using example data from both our United States and International formats of data. We want this to replicate the data format we see on Olympus, that way we can be sure if the functions and synthetic populations are working here, they are sure to work on Olympus. Because the Olympus format of the data isn't formatted the same way we would format a typical R data-set, we can include the data in our source (Github) package, and use `.Rbuildignore` to make sure this data isn't carried through to the insallation process. 

To add the example data, simply use:

```{r}
devtools::use_build_ignore("data-raw")
```

Now, in this folder we can also include scripts which convert this raw data into more efficient `.rdata` files, which can be used for testing/verification of other aspects of spew down the road.  

## Including dependencies in our package 

One critical aspect of developing and R package is learning how to incorporate dependencies from other packages, while not adversely effecting other users' R landscape. For example, in spew, we rely on certain packages for dealing with shapefiles, linking records, etc... Instead of putting `library(sp)` in the first line of the function, the preferred method is to add the `sp` package to our list of dependencies, and this way to will avoid errors down the road. Fortunately, devtools gives an easy way to add dependencies to our `DESCRIPTION` file, which contains all of the metadata needed for using our package. 

To add a necessary package dependency to spew, simply use:

```{r, eval=FALSE}
devtools::use_package("sp")
```

This means that anyone using the spew package MUST have the sp package installed as well. For a less stringest requirement, (ie: only some use cases need the record linkage package), we can simply suggest a dependency:

```{r, eval=FALSE}
devtools::use_package("RecordLinkage", "Suggests")
```

Note for dependendies with other packages, you can use the `::` command, which specifies which package a particular function is coming from. For example, when we use the slot function we can write `methods::slot(shapefile, "polygons")`


# About the Program 

Right now, we have divided spew into three main sections, each of which will be described in this section. The three sections are:

1. Read 
2. Format
3. Make 

## Read
The read.R functions have a clear purpose: read in all the necessary data for a a particular run of spew. Because the data comes in many different formats, places, etc, this is the function which will ideally handle al of these for us. For our purposes, most of the data is in a particular structure on Olympus, so we specify particular data groups as inputs to make everything work easier. But in principal, once should be able to specify filepaths to each data source, and read_data should be able to access them. 

For our program to become a general engine for synthetic population generation, we need it to be able to work for any arbitrary data source people have. To address this, we also have functions in read_data which standardize the data. At a high level, this means that any data source we use needs to have some columns, ID, etc... which allows it to be linked with the other data sources. For spew, the most important thing is that the locations of each data source can be combined, and for this we standardized each data source to have the colums/elements:  

1. place_id 
2. puma_id 

The idea here is that we are generating a population for n individual places, which we specify with the place ID. In some situations, such as the US, the pums data we recieve is more granular, and we can subset it to obtain more accurate populations. We designate this subset with the puma_id, which is used later down the road in the `make_data` functions. Thus, every data source coming through read_data should not only be loaded into R's memory, but it should also have an identifier indicating which part of it corresponds to the place id, and `puma_id` (if necessary). 


## Format 


## Make 


## Running SPEW on Olympus 

Since our project is all hosted on github, we should in theory be able to simply install the package on Olympus, and run the `spew` function with the appropriate inputs just as we would with any other package. To do this, we need to have devtools installed (which Jay has nicely done for us) on the headnode. Furthermore, we must make sure that we have the most recent version of R loaded, using the command:

```{r, include=FALSE}
# module load r/3.2.1
```

This will replace the default R 3.1 with the latest versions. From here, to download spew onto Olympus we type in R to enter an R session, then the following:

```{r, include=FALSE}
  library(devtools)
  devtools::install_github(repo="leerichardson/spew")
  library(spew)
```

Note that this certainly did not run smoothly for me the first time I tried it. There were some issues downloading spew into the correct library, which in devtools by default goes to the system packages available for all users. Next, I had to remove some lines from the DESCRIPTION file that were causing errors, and remember to "export" the functions I wanted to run, to the NAMESPACE of the package. I'm mainly writing this here so I don't forget, and here are the commands I ended up using:  

```{r, include=FALSE, eval=FALSE}
library(devtools)
library(httr)
personal_lib <- .libPaths()[2]
with_libpaths(new = personal_lib, install_github("leerichardson/spew"))
library(spew)
?generate_spew
```

Other times, you will realize that spew as is doesn't quite work, and we need to make changes to it. In this case, to run on Olympus we will need to remove spew as is, and re-install it:

```{r, include=FALSE, eval=FALSE}
personal_lib <- .libPaths()[2]
remove.packages(pkgs = "spew", lib = personal_lib)
```

Then we can reset R and delete our current workspace, and things should now work as expected. 

Now, we can call the `generate_spew` function on Olympus for a given input directory, and it works just as it does on our local computers. 

Finally, once you re-load the package, it's useful to debug it in Interactive mode on Olympus, just to make sure things are working as expected. you can submit an interactive job (ie: Log onto one of the compute nodes) with the command `qsub -I -l walltime=3:00:00`. To save time, I usually run the US version on State 01 to find bus:

```{r, include=FALSE, eval=FALSE}
library(spew)
library(maptools)
library(foreach)
library(doParallel)
options(error = recover)

in_dir <- "/data/shared_group_data/syneco/input/west/north_america/united_states/72"
out_dir <- "/data/shared_group_data/syneco/outputs/spew/west/north_america/united_states/72"
folders <- list(pop_table = "popTables", 
                     pums = "pums/2013", 
                     schools = "schools", 
                     lookup = "tables", 
                     shapefiles = "tiger", 
                     workplaces = "workplaces")

folders <- list(pop_table = "popTables", 
                     pums = "pums/2013",  
                     lookup = "tables", 
                     shapefiles = "tiger")

generate_spew(input_dir = in_dir, output_dir = out_dir, folders = folders, 
              parallel = FALSE, data_group="US", sampling_type="uniform")
```

## Shell Scripting 
In our use of spew, we will make a lot of use of the Olympus infrastructure to speed up our calculations. In order to use these capabilities, we will need to submit jobs to olympus which request the requisite amount of processors and time. To submit jobs and interact with Olympus, we make use of shell scripts. 

Right now, all of the scripts are located in the `/data/shared_group_data/syneco/spew` directory. All of the files in here have a specific purpose, but the main idea is this. There's a lookup table in the `/input/spew_lookup.csv` directory, which contains all of the inputs necessary to run spew. What we want to do, is to subset this lookup table corresponding to whichever synthetic population we are interested in generated, grab the inputs, and then use these inputs to call spew. This is what the `/spew/run_spew.sh` does, we pass it a query, and this grabs the appropriate inputs from the lookup table. Then, it submits a job for each row of the lookup table that remains after the subset. 

For example, to run a particular state, we can call:
`/data/shared_group_data/syneco/spew/run_spew.sh "which(lookup[, 'state_dirs'] == '56')"`

And to run the entire United States, we can run:
`/data/shared_group_data/syneco/spew/run_spew.sh "which(lookup[, 'country'] == 'united_states')"`

In the case of the United States, what happens is that we subset the lookup table for all of the states, and then submit a job which calls spew for each individual state. The result of these calls to spew are saved in the `/outputs/spew/` directory. 

This system can definitely be improved, and will for sure undergo improvements as we move forward. Ideally, we can keep adding to the spew package ways of handling new types of data, update the parameters necesary for these calls in the lookup table, and systematically generate synthetic populations in the same way for every location upon which we have data. 
